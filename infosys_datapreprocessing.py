# -*- coding: utf-8 -*-
"""infosys_datapreprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PJmbnqV5YEqw0eNMM8ZyHQY4_1r6jwt2

# Details of pre processing
"""

import pandas as pd
df = pd.read_csv('/content/Churn_ Data.csv')
df

"""step1:- Convert data type of variables which are misclassified"""

print(df.dtypes)

print(df.dtypes.unique())

def count_misclassified_columns(df):
    misclassified_count = sum(df.dtypes != 'float64')
    return misclassified_count

misclassified_count = count_misclassified_columns(df)
print("Number of misclassified columns:", misclassified_count)

def count_misclassified_columns(df):
    misclassified_count = sum(df.dtypes != 'int64')
    return misclassified_count

misclassified_count = count_misclassified_columns(df)
print("Number of misclassified columns:", misclassified_count)

def convert_misclassified_columns(df):
    misclassified_columns = df.columns[df.dtypes != 'float64']

    df[misclassified_columns] = df[misclassified_columns].astype('float64')

    return df

df_corrected = convert_misclassified_columns(df)

print(df_corrected)

df

def count_misclassified_columns(df):
    misclassified_count = sum((df.dtypes != 'float64') & (df.dtypes != 'int64'))
    return misclassified_count

misclassified_count = count_misclassified_columns(df)
print("Number of misclassified columns:", misclassified_count)

"""step 2:- Remove duplicate value"""

def count_duplicate_records(df):
    duplicate_count = df.duplicated().sum()
    return duplicate_count

duplicate_count = count_duplicate_records(df)
print("Number of duplicate records:", duplicate_count)

df.drop_duplicates(inplace=True)

"""step 3:- Removing Unique value variables"""

def count_unique_value_variables(df):
    unique_value_variables = sum(df.nunique() == len(df))
    return unique_value_variables

unique_value_variables_count = count_unique_value_variables(df)
print("Number of unique value variables:", unique_value_variables_count)

def remove_unique_value_variables(df):
    unique_value_variables = []

    for column in df.columns:
        if df[column].nunique() == len(df):
            unique_value_variables.append(column)

    df = df.drop(columns=unique_value_variables)

    return df

df = remove_unique_value_variables(df)
print("DataFrame with unique value variables removed:")
print(df)

unique_cols = df.columns[df.nunique() == 1]
df = df.drop(unique_cols, axis=1)
print(df)

"""step 4:-Removing Zero variance Variables"""

def check_zero_variance_variables(df):
    zero_variance_variables = []

    for column in df.columns:
        if df[column].var() == 0:
            zero_variance_variables.append(column)

    return zero_variance_variables

zero_variance_variables = check_zero_variance_variables(df)
print("Zero variance variables:", zero_variance_variables)

zero_var_cols = df.columns[df.var() == 0]
df = df.drop(zero_var_cols, axis=1)

def remove_zero_variance_variables(df):
    zero_variance_variables = [col for col in df.columns if df[col].nunique() == 1]

    df = df.drop(columns=zero_variance_variables)

    return df

df_removed_zero_variance = remove_zero_variance_variables(df)
print("DataFrame with zero variance variables removed:")
print(df_removed_zero_variance)

"""step 5:- Treatment outlier"""

from scipy.stats import zscore
def check_outliers_zscore(df, threshold=3):
    z_scores = df.apply(zscore)
    outliers = (z_scores.abs() > threshold).any(axis=1)
    df_outliers = df[outliers]

    return df_outliers

outliers_zscore = check_outliers_zscore(df)
print("Outliers identified using z-score method:")
print(outliers_zscore)

import numpy as np
def treat_outliers_winsorization(df, lower_limit=0.25, upper_limit=0.75):
    for column in df.columns:
        lower_bound = df[column].quantile(lower_limit)
        upper_bound = df[column].quantile(upper_limit)
        df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])
        df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])

    return df

df_winsorized = treat_outliers_winsorization(df)
print("DataFrame with outliers treated using Winsorization:")
print(df_winsorized)

def standardize_data(df):
    mean = df.mean()
    std_dev = df.std()

    df = (df - mean) / std_dev

    return df

# Standardize data using +/- 3 Sigma approach
standardized_df = standardize_data(df)

outliers = (standardized_df.abs() > 3).any(axis=1)

print("Standardized DataFrame:")
print(standardized_df)

print("\nIdentified outliers:")
print(df[outliers])

"""step 6:- Missing Value Treatment"""

missing_values_count = df.isnull().sum()
print("Number of missing values in each column:")
print(missing_values_count)

def missing_value_imputation(df, method='mean'):
    if method == 'mean':

        df = df.fillna(df.mean())
    elif method == 'median':

        df = df.fillna(df.median())
    elif method == 'mode':

        df = df.fillna(df.mode().iloc[0])
    else:
        raise ValueError("Invalid imputation method. Choose from 'mean', 'median', or 'mode'.")

    return df

df_imputed_mean = missing_value_imputation(df, method='mean')
print("DataFrame with missing values imputed using mean:")
print(df_imputed_mean)

"""step 7:- Removing the highly correlated variables"""

def count_highly_correlated_variables(df, threshold=0.8):
    corr_matrix = df.corr().abs()

    mask = (corr_matrix >= threshold) & (corr_matrix < 1)

    count = sum(mask.sum())

    return count

correlated_count = count_highly_correlated_variables(df)
print("Number of highly correlated variables:", correlated_count)

def remove_highly_correlated_variables(df, threshold=0.80):
    corr_matrix = df.corr().abs()

    mask = (corr_matrix >= threshold) & (corr_matrix < 1)

    features_to_keep = [column for column in mask.columns if not any(mask[column])]

    df = df[features_to_keep]

    return df

df = remove_highly_correlated_variables(df)
print("DataFrame with highly correlated variables removed:")
print(df)

df

"""***step 8:-Multicollinearity (VIF > 5)***"""

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(df):

    vif_data = pd.DataFrame()
    vif_data["Variable"] = df.columns
    vif_data["VIF"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]

    return vif_data

vif_df = calculate_vif(df)
print("VIF values for each variable:")
print(vif_df)

variables_with_high_vif_count = (vif_df["VIF"] > 5).sum()
print("Number of variables with VIF greater than 5:", variables_with_high_vif_count)

from statsmodels.stats.outliers_influence import variance_inflation_factor

def calculate_vif(df):
    vif_data = pd.DataFrame()
    vif_data["Variable"] = df.columns
    vif_data["VIF"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]

    return vif_data

vif_df = calculate_vif(df)
print("VIF values for each variable:")
print(vif_df)

high_vif_variables = vif_df[vif_df['VIF'] > 5]['Variable']

df = df.drop(high_vif_variables, axis=1)

print("DataFrame after removing variables with high VIF values:")
print(df)

df

"""# step-9 model preparation"""

X = df.drop('target', axis=1)
y = df['target']

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assume you have features (X) and target variable (y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

y_pred = log_reg.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print(accuracy_score(y_train, log_reg.predict(X_train)))

log_reg_acc = accuracy_score(y_test, log_reg.predict(X_test))
print(log_reg_acc)

print(confusion_matrix(y_test, y_pred))

print(classification_report(y_test, y_pred))

from sklearn.tree import DecisionTreeClassifier
dt_model = DecisionTreeClassifier(random_state=42)

dt_model.fit(X_train, y_train)

y_pred = dt_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Decision Tree Classifier Accuracy: {accuracy:.3f}")
print(f"Decision Tree Classifier Precision: {precision:.3f}")
print(f"Decision Tree Classifier Recall: {recall:.3f}")
print(f"Decision Tree Classifier F1-score: {f1:.3f}")

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(random_state=42)

rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Random Forest Accuracy: {accuracy:.3f}")
print(f"Random Forest Precision: {precision:.3f}")
print(f"Random Forest Recall: {recall:.3f}")
print(f"Random Forest F1-score: {f1:.3f}")



